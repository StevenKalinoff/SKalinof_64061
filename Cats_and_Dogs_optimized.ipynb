{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "!unzip -qq /content/gdrive/MyDrive/cats_vs_dogs_small_dataset.zip\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import Sequential\n",
        "from keras.layers import Dense,Conv2D,MaxPooling2D,Flatten,BatchNormalization,Dropout\n",
        "from keras import layers\n",
        "from keras import models\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "41rI-Z0aFIS7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Code to remove the data and folder created from a previous run of the data\n",
        "!rm -rf \"cats_vs_dogs_small\"\n",
        "#!rm -rf \"cats_vs_dogs_small_dataset\""
      ],
      "metadata": {
        "id": "7NWcKp9Z-a6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the following code to partition the data into train/test/validation set:\n",
        "import os, shutil, pathlib\n",
        "\n",
        "original_dir = pathlib.Path(\"cats_vs_dogs_small_dataset\")\n",
        "new_base_dir = pathlib.Path(\"cats_vs_dogs_small\")\n",
        "\n",
        "def make_subset(subset_name, start_index, end_index):\n",
        "    count = 0\n",
        "    for category in (\"cat\", \"dog\"):\n",
        "        dir = new_base_dir / subset_name / category\n",
        "        os.makedirs(dir)\n",
        "        fnames = [f\"{i}.jpg\" for i in range(start_index, end_index)]\n",
        "        for fname in os.listdir(original_dir / category)[start_index:end_index]:\n",
        "            shutil.copyfile(original_dir / category / fname, dir / fname)\n",
        "            count += 1\n",
        "    return count\n",
        "\n",
        "train_count = make_subset(\"train\", start_index=0, end_index=1500)\n",
        "validation_count = make_subset(\"validation\", start_index=1500, end_index=1750)\n",
        "test_count = make_subset(\"test\", start_index=1750, end_index=2000)\n",
        "\n",
        "print(f\"Number of images in train subset: {train_count}\")\n",
        "print(f\"Number of images in validation subset: {validation_count}\")\n",
        "print(f\"Number of images in test subset: {test_count}\")\n",
        "\n",
        "train_ds = keras.utils.image_dataset_from_directory(\n",
        "    directory = pathlib.Path(\"cats_vs_dogs_small/train\"),\n",
        "    labels='inferred',\n",
        "    label_mode = 'int',\n",
        "    batch_size=100,\n",
        "    image_size=(150,150)\n",
        ")\n",
        "\n",
        "validation_ds = keras.utils.image_dataset_from_directory(\n",
        "    directory = pathlib.Path(\"cats_vs_dogs_small/test\"),\n",
        "    labels='inferred',\n",
        "    label_mode = 'int',\n",
        "    batch_size=100,\n",
        "    image_size=(150,150)\n"
      ],
      "metadata": {
        "id": "dYsvEGDpaMFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize\n",
        "def process(image,label):\n",
        "    image = tf.cast(image/255. ,tf.float32)\n",
        "    return image,label\n",
        "\n",
        "train_ds = train_ds.map(process)\n",
        "validation_ds = validation_ds.map(process)\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu',\n",
        "input_shape=(150, 150, 3)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Flatten())\n",
        "#model.add(layers.Dropout(0.3))  # Added dropout here\n",
        "model.add(layers.Dense(512, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
        "history = model.fit(train_ds,epochs=10,validation_data=validation_ds)\n",
        "\n",
        "plt.plot(history.history['accuracy'],color='red',label='train')\n",
        "plt.plot(history.history['val_accuracy'],color='blue',label='validation')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0ENXBOMzKaxm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}