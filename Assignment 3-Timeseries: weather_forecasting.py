# -*- coding: utf-8 -*-
"""Weather Forecasting

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Vow4SiTxfmL-b7cBzXUvSRmLLVeOL0Z6
"""

from google.colab import drive
drive.mount('/content/gdrive')

#!pip install tensorflow
import numpy as np
import tensorflow as tf
from tensorflow import keras
from keras import Sequential
from keras.layers import Dense,Conv2D,MaxPooling2D,Flatten,BatchNormalization,Dropout
from keras import layers
from keras import models
import matplotlib.pyplot as plt
from tensorflow.keras.layers import GRU, Dense, Dropout, Bidirectional, LSTM
from tensorflow.keras.optimizers import RMSprop
from tensorflow.keras.callbacks import ModelCheckpoint
from tensorflow.keras.models import load_model

# Code to remove the data and folder created from a previous run of the data
!rm -rf "jena_climate_2009_2016.csv"

!unzip -qq /content/gdrive/MyDrive/jena_climate_2009_2016.csv.zip

#Climate data
fname = 'jena_climate_2009_2016.csv'
f = open(fname)
data = f.read()
f.close()
lines = data.split('\n')
header = lines[0].split(',')
lines = lines[1:]
print(header)
print(len(lines))

import numpy as np
temperature = np.zeros((len(lines),))
raw_data = np.zeros((len(lines), len(header) - 1))
for i, line in enumerate(lines):
    values = [float(x) for x in line.split(",")[1:]]
    temperature[i] = values[1]
    raw_data[i, :] = values[:]

# Splitting data
num_train_samples = int(0.5 * len(raw_data))
num_val_samples = int(0.25 * len(raw_data))
num_test_samples = len(raw_data) - num_train_samples - num_val_samples
print("num_train_samples:", num_train_samples)
print("num_val_samples:", num_val_samples)
print("num_test_samples:", num_test_samples)

#Data statistics
mean = raw_data[:num_train_samples].mean(axis=0)
raw_data -= mean
std = raw_data[:num_train_samples].std(axis=0)
raw_data /= std

int_sequence = np.arange(10)
dummy_dataset = keras.utils.timeseries_dataset_from_array(
    data=int_sequence[:-3],
    targets=int_sequence[3:],
    sequence_length=3,
    batch_size=2,
)

for inputs, targets in dummy_dataset:
    for i in range(inputs.shape[0]):
        print([int(x) for x in inputs[i]], int(targets[i]))

sampling_rate = 6
sequence_length = 120
delay = sampling_rate * (sequence_length + 24 - 1)
batch_size = 256

train_dataset = keras.utils.timeseries_dataset_from_array(
    raw_data[:-delay],
    targets=temperature[delay:],
    sampling_rate=sampling_rate,
    sequence_length=sequence_length,
    shuffle=True,
    batch_size=batch_size,
    start_index=0,
    end_index=num_train_samples)

val_dataset = keras.utils.timeseries_dataset_from_array(
    raw_data[:-delay],
    targets=temperature[delay:],
    sampling_rate=sampling_rate,
    sequence_length=sequence_length,
    shuffle=True,
    batch_size=batch_size,
    start_index=num_train_samples,
    end_index=num_train_samples + num_val_samples)

test_dataset = keras.utils.timeseries_dataset_from_array(
    raw_data[:-delay],
    targets=temperature[delay:],
    sampling_rate=sampling_rate,
    sequence_length=sequence_length,
    shuffle=True,
    batch_size=batch_size,
    start_index=num_train_samples + num_val_samples)

for samples, targets in train_dataset:
    print("samples shape:", samples.shape)
    print("targets shape:", targets.shape)
    break

def evaluate_naive_method(dataset):
    total_abs_err = 0.
    samples_seen = 0
    for samples, targets in dataset:
        preds = samples[:, -1, 1] * std[1] + mean[1]
        total_abs_err += np.sum(np.abs(preds - targets))
        samples_seen += samples.shape[0]
    return total_abs_err / samples_seen

print(f"Validation MAE: {evaluate_naive_method(val_dataset):.2f}")
print(f"Test MAE: {evaluate_naive_method(test_dataset):.2f}")

# Baseline test
inputs = keras.Input(shape=(sequence_length, raw_data.shape[-1]))
x = layers.Flatten()(inputs)
x = layers.Dense(16, activation="relu")(x)
outputs = layers.Dense(1)(x)
model = keras.Model(inputs, outputs)

callbacks = [
    keras.callbacks.ModelCheckpoint("jena_dense.keras",
                                    save_best_only=True)
]
model.compile(optimizer="rmsprop", loss="mse", metrics=["mae"])
history = model.fit(train_dataset,
                    epochs=4,
                    validation_data=val_dataset,
                    callbacks=callbacks)

model = keras.models.load_model("jena_dense.keras")
print(f"Test MAE: {model.evaluate(test_dataset)[1]:.2f}")

import matplotlib.pyplot as plt
loss = history.history["mae"]
val_loss = history.history["val_mae"]
epochs = range(1, len(loss) + 1)
plt.figure()
plt.plot(epochs, loss, "bo", label="Training MAE")
plt.plot(epochs, val_loss, "b", label="Validation MAE")
plt.title("Training and validation MAE")
plt.legend()
plt.show()
# 4th epoch has lowest point of MAE

inputs = keras.Input(shape=(sequence_length, raw_data.shape[-1]))
x = layers.LSTM(16)(inputs)
outputs = layers.Dense(1)(x)
model = keras.Model(inputs, outputs)

callbacks = [
    keras.callbacks.ModelCheckpoint("jena_lstm.keras",
                                    save_best_only=True)
]
model.compile(optimizer="rmsprop", loss="mse", metrics=["mae"])
history = model.fit(train_dataset,
                    epochs=10,
                    validation_data=val_dataset,
                    callbacks=callbacks)

model = keras.models.load_model("jena_lstm.keras")
print(f"Test MAE: {model.evaluate(test_dataset)[1]:.2f}")

#Grabbing Data for output sequence
timesteps = 100
input_features = 32
output_features = 64
inputs = np.random.random((timesteps, input_features))
state_t = np.zeros((output_features,))
W = np.random.random((output_features, input_features))
U = np.random.random((output_features, output_features))
b = np.random.random((output_features,))
successive_outputs = []
for input_t in inputs:
    output_t = np.tanh(np.dot(W, input_t) + np.dot(U, state_t) + b)
    successive_outputs.append(output_t)
    state_t = output_t
final_output_sequence = np.stack(successive_outputs, axis=0)

inputs = keras.Input(shape=(steps, num_features))
x = layers.SimpleRNN(16, return_sequences=True)(inputs)
x = layers.SimpleRNN(16, return_sequences=True)(x)
outputs = layers.SimpleRNN(16)(x)

#implementing callback to save the best version
inputs = keras.Input(shape=(sequence_length, raw_data.shape[-1]))
x = layers.LSTM(32, recurrent_dropout=0.25)(inputs)
x = layers.Dropout(0.5)(x)
outputs = layers.Dense(1)(x)
model = keras.Model(inputs, outputs)

callbacks = [
    keras.callbacks.ModelCheckpoint("jena_lstm_dropout.keras",
                                    save_best_only=True)
]
model.compile(optimizer="rmsprop", loss="mse", metrics=["mae"])
history = model.fit(train_dataset,
                    epochs=6, # MAE drops after epoch 6 dropping extra epochs for time saving
                    validation_data=val_dataset,
                    callbacks=callbacks)
inputs = keras.Input(shape=(sequence_length, num_features))
x = layers.lstm(32, recurrent_dropout=0.2, unroll=True)(inputs)

model = keras.models.load_model("jena_lstm.keras")
print(f"Test MAE: {model.evaluate(test_dataset)[1]:.2f}")

import matplotlib.pyplot as plt
loss = history.history["mae"]
val_loss = history.history["val_mae"]
epochs = range(1, len(loss) + 1)
plt.figure()
plt.plot(epochs, loss, "bo", label="Training MAE")
plt.plot(epochs, val_loss, "b", label="Validation MAE")
plt.title("Training and validation MAE")
plt.legend()
plt.show()

def build_gru_model(input_shape, recurrent_dropout_rate=0.25, dropout_rate=0.25):
    model = Sequential([
        GRU(16, recurrent_dropout=recurrent_dropout_rate, return_sequences=True, input_shape=input_shape),
        GRU(16, recurrent_dropout=recurrent_dropout_rate),
        Dropout(dropout_rate),
        Dense(1)
    ])
    return model

def build_bidirectional_lstm_model(input_shape):
    model = Sequential([
        Bidirectional(LSTM(16, input_shape=input_shape)),
        Dense(1)
    ])
    return model

# GRU
gru_model = build_gru_model((sequence_length, raw_data.shape[-1]))
gru_model.compile(optimizer=RMSprop(), loss='mse', metrics=['mae'])


checkpoint_callback = ModelCheckpoint("best_model_gru.h5", save_best_only=True)

gru_history = gru_model.fit(train_dataset, epochs=6, validation_data=val_dataset, callbacks=[checkpoint_callback])


best_gru_model = load_model("best_model_gru.h5")

# Evaluate
test_mae_gru = best_gru_model.evaluate(test_dataset)[1]
print(f"GRU Test MAE: {test_mae_gru:.2f}")

# Compile and train the bidirectional LSTM model
lstm_model = build_bidirectional_lstm_model((sequence_length, raw_data.shape[-1]))
lstm_model.compile(optimizer=RMSprop(), loss='mse', metrics=['mae'])

lstm_history = lstm_model.fit(train_dataset, epochs=6, validation_data=val_dataset)

# Plot
plt.plot(gru_history.history['mae'], 'bo', label='GRU Training MAE')
plt.plot(gru_history.history['val_mae'], 'b', label='GRU Validation MAE')
plt.plot(lstm_history.history['mae'], 'ro', label='LSTM Training MAE')
plt.plot(lstm_history.history['val_mae'], 'r', label='LSTM Validation MAE')
plt.title('Training and Validation MAE')
plt.xlabel('Epochs')
plt.ylabel('MAE')
plt.legend()
plt.show()

# Running with test data
best_gru_model = load_model("best_model_gru.h5")

test_loss_gru, test_mae_gru = best_gru_model.evaluate(test_dataset)
print(f"GRU Test Loss: {test_loss_gru:.2f}")
print(f"GRU Test MAE: {test_mae_gru:.2f}")

test_loss_lstm, test_mae_lstm = lstm_model.evaluate(test_dataset)
print(f"LSTM Test Loss: {test_loss_lstm:.2f}")
print(f"LSTM Test MAE: {test_mae_lstm:.2f}")
